
<PLATFORM-NAME>安装部署指南
===================================================

## 简介

  本文档详细描述了在制造业产品线上安装部署依瞳人工智能平台运行环境的安装、升级以及卸载等流程的具体操作指导。
  运行环境是指实际运行用户开发的开发模型和执行推理的环境，要求实际安装昇腾AI处理器。

**读者对象**

  本文档主要适用于以下人员：

  * 安装部署人员
  * 平台维护管理员或技术支持人员

**适用范围**

  本文档仅适用于部署 1台Master + 多台Worker的集群，其中Master服务器硬件架构为x86-amd64，Worker服务器硬件架构可以是amd64或arm64.

  本文档中的配置实例为：

  `1*amd64 Master + 1*amd64 NPU-Worker + 1*arm64 NPU-Worker`

**命令行格式约定**

  |格式              | 意义                                                           |
  |------------------|----------------------------------------------------------------|
  |**粗体**          |命令行关键字（命令中保持不变、必须照输的部分）采用加粗字体表示。|
  |*斜体*            |命令行参数（命令中必须由实际值进行替代的部分）采用斜体表示。    |
  |[ ]               |表示用“[ ]”括起来的部分在命令配置时是可选的。                   |
  |{ x \| y \| ... } |表示从两个或多个选项中选取一个。                                |
  |[ x \| y \| ... ] |表示从两个或多个选项中选取一个或者不选。                        |
  |{ x \| y \| ... }*|表示从两个或多个选项中选取多个，最少选取一个，最多选取所有选项。|
  |[ x \| y \| ... ]*|表示从两个或多个选项中选取多个或者不选。                        |
  |&<1-n>            |表示符号&前面的参数可以重复1～n次。                             |
  |#                 |由“#”开始的行表示为注释行。                                     |

**修改记录**

  |文档版本| 发布日期        |  修改说明|
  |--------|-----------------|----------|
  |v0.5.0  | *2021-03-23*    |草稿      |

**安装须知**

  1. 文档中的配置和软件包的请根据实际版本发布情况沟通确认，
  2. 实际部署中的设备、网络和驱动等请跟技术支持人员沟通确认。

## 系统要求

  <PLATFORM_NAME> 为兼容昇腾910 AI处理器、Nvidia GPU处理器，Atlas 系列服务器发布的驱动包，使用以下指定的host操作系统和内核，详细安装信息如下表所示。

  |硬件形态                               |host操作系统版本 |host操作系统内核版本 |
  |---------------------------------------|-----------------|---------------------|
  |amd64 CPU服务器                        |Ubuntu 18.04.1   |4.15.0-99-generic    |
  |arm64+Atlas 500 AI加速模块（型号 3000）|Ubuntu 18.04.1   |4.15.0-99-generic    |
  |amd64+Atlas 500 AI加速模块（型号 3000）|Ubuntu 18.04.1   |4.15.0-99-generic    |

### 安装前准备

  为保障安装能够顺利进行，需要检查软硬件环境，准备所需的安装包。

#### 准备软件包

  1. CANN 驱动、固件和工具包
  2. 安装环境apt依赖包
  3. 安装工具ansible及其配置和依赖包
  4. 平台基础镜像、业务镜像、预置模型镜像
  5. 预置模型、原始数据集等 

#### 检验软件包完整性

  建议在传输前计算所有软件包的大小，哈希值，标明相关文件内容的版本信息等；以便在安装环境中检查下载的软件包是否完整、一致。

  为了防止CANN软件包在传递过程或存储期间被恶意篡改，下载软件包时需下载对应的数字签名文件用于完整性验证。

  在软件包下载之后，请参考《OpenPGP签名验证指南》，对从Support网站下载的软件包进行PGP数字签名校验。如果校验失败，请不要使用该软件包，先联系华为技术支持工程师解决。

  使用软件包安装/升级之前，也需要按上述过程先验证软件包的数字签名，确保软件包未被篡改。

  企业客户请访问：https://support.huawei.com/enterprise/zh/tool/pgp-verify-TL1000000054

**安装所需工具和镜像包列表**

  |组件               |	repo                                                                         |	版本           |
  |:------------------|:-----------------------------------------------------------------------------|:----------------|
  |AIArts前端	        |https://apulis-gitlab.apulis.cn/apulis/AIArts.git 	                           |v3.0.0-songshanhu|
  |AIArts后端	        |https://apulis-gitlab.apulis.cn/apulis/aiarts-pro-backend.git 	               |develop          |
  |用户管理前端	      |https://apulis-gitlab.apulis.cn/apulis/addon_custom_user_group_dashboard.git	 |v1.6.0           |
  |用户管理后端 	    |https://apulis-gitlab.apulis.cn/apulis/addon_custom_user_dashboard_backend.git |v1.6.0           |
  |专家系统	          |https://apulis-gitlab.apulis.cn/apulis/DLWorkspace.git	                       |v2.0.0           |
  |数据集管理后台	    |https://apulis-gitlab.apulis.cn/apulis/dataset-manager-backend	               |master           |
  |松山湖原始数据对接	|https://apulis-gitlab.apulis.cn/apulis/ipc-data-loader	                       |master           |
  |数据标注（前后端）	|https://apulis-gitlab.apulis.cn/apulis/cvat	                                   |v0.3.0-apulis    |
  |安装部署	          |https://apulis-gitlab.apulis.cn/apulis/InstallationYTung	                     |                 |
  |预置包导入脚本	    |https://apulis-gitlab.apulis.cn/apulis/industrial-scene-toolkit	               |master           |
  |抠图功能	          |https://apulis-gitlab.apulis.cn/apulis/crop-image-dataset	                     |master           |


### 硬件和网络规划


  |   节点| 访问IP            |访问端口| 设备         |架构   |业务IP       |
  |-------|-------------------|--------|--------------|-------|-------------|
  |Worker |<HOST-OPEN-ADDRESS>| <PORT> |atlas 800-9000| arm64 |192.168.1.183|
  |Worker |<HOST-OPEN-ADDRESS>| <PORT> |atlas 800-3000| amd64 |192.168.1.107|
  |Master |<HOST-OPEN-ADDRESS>| <PORT> |atlas 800-3010| amd64 |192.168.1.113|

  **必须开放的对外访问端口： 80, 443, 20000~40000**

  *集群的master作为ansible的管理节点!*

  **网络拓扑示意图**

  <div style="text-align: center;">

  ![netmap](../../../../docs/img/netmap/netmap-chengdulanqu.png)
  </div>

## 预安装环境

  1. 安装指定的主机操作系统和网卡驱动（如果已经按照，则要检查系统版本、驱动版本是否满足要求）
  2. 配置业务面网络，参数面网络和开放访问端口

### 创建运行用户


  使用root用户安装驱动，但安装完之后要求使用非root用户运行，所以安装前需要先创建运行用户。

  * 使用NPU 20.0.X系列版本软件包安装时，创建的运行用户必须是HwHiAiUser。安装软件包时可以直接使用该运行用户，默认即为HwHiAiUser。
  * 使用NPU 20.1.X和NPU 20.2.X系列版本软件包安装时，如果创建的用户是非HwHiAiUser，安装驱动软件包时需要指定运行用户（通过--install-username=username --install-usergroup=usergroup参数指定），请参见如下方法创建。安装固件软件包不支持指定运行用户名和用户组，共用Driver的运行用户名和用户组。
  
  ```bash
  groupadd <usergroup> 
  useradd -u <uid> -g <usergroup> -d /home/username -m <username> -s /bin/bash
  # 执行以下命令设置密码。
  passwd <usergroup>
  ```

### 配置免密登录

1. 登录管理节点并初始化SSH Key

  `ssh-keygen -t xxxxxx -C "<EMAIL-ACCOUNT>"`

2. 将管理节点的公钥拷贝到所有被管理节点的机器上：

  ```
  # ssh-copy-id -i ~/.ssh/id_rsa.pub root@<MASTER-PRIVATE-IP> 
  cp -f ~/.ssh/id_*.pub ~/.ssh/id_rsa.pub
  cp -f ~/.ssh/id_%S  ~/.ssh/id_rsa

  chmod 644 ~/.ssh/id_rsa.pub
  ssh-copy-id -i ~/.ssh/id_rsa.pub root@<MASTER-PRIVATE-IP>
  ssh-copy-id -i ~/.ssh/id_rsa.pub root@<WORKER01-PRIVATE-IP>
  ssh-copy-id -i ~/.ssh/id_rsa.pub root@<WORKER02-PRIVATE-IP>
  ```

### 获取部署工具和配置

  * 一般将Master作为管理节点，推荐安装目录放在`/home/`。
  * 获取指定版本的配置包（可联系技术支持获取完整的离线安装包）

  ```
  cd /home && git clone -b <RELEASE-VERSION> https://apulis-gitlab.apulis.cn/apulis/InstallationYTung.git && cd InstallationYTung 
  ```


### 检查管理节点能否ping通所有的被管理节点

  ```
  # cd /home/InstallationYTung
  ansible all -i hosts -m ping

  The authenticity of host '192.168.1.113 (192.168.1.113)' can't be established.
  ECDSA key fingerprint is SHA256:5q6sr8SMfMco15RtB/cbVAIaTs3EvaT3GXq5KvZzOCg.
  Are you sure you want to continue connecting (yes/no)? 192.168.1.183 | SUCCESS => {
      "changed": false,
      "ping": "pong"
  }
  <HOST-IP> | SUCCESS => {
      "changed": false,
      "ping": "pong"
  }

  ```

### 导入安装依赖包


  将事先准备好的安装盘中的`bin/`的安装包拷贝到管理节点的 `/home/InstallationYTung/bin`

  `cp -r <Install-PACKAGES/<VERSION>/bin/*> /home/InstallationYTung/bin`

### 导入离线安装镜像包

  将事先准备好的安装盘中的`resources/images-pushed/`下安装镜像包拷贝到管理节点的InstallationYTung/resources/images路径下：

  `cp -r <Install-PACKAGES/<VERSION>/resources/images-pushed/*> /home/InstallationYTung/resources/images`

  **注意**

  - images：存放即将要上传到集群的的镜像的tar包，以及harbor的离线安装包。
  - images-pushed：在后面执行部署harbor的步骤之后， images目录下的镜像tar包会被load->tag->create(manifest)->annotate(manifest)->push(manifest)，推送到harbor之后，images目录下的镜像tar包会被mv到images-pushed（已推送）目录下。
  - 镜像的tar包的名称并不重要，重要的是镜像tar包被load之后，镜像name与tag是跟InstallationYTung/group_vars/all.yaml里面的一致的。当然，tar包名称最好能标识镜像name和tag。


### 安装ansible及其依赖包
    
  * 离线安装

    `dpkg -i InstallationYTung/apt/ansible.deb` 

  * 在线安装ansible

  ```
  sudo cp -a /etc/apt/sources.list /etc/apt/sources.list.bak && sudo sed -i "s@http://.*archive.ubuntu.com@http://mirrors.huaweicloud.com@g" /etc/apt/sources.list && sudo sed -i "s@http://.*security.ubuntu.com@http://mirrors.huaweicloud.com@g" /etc/apt/sources.list
  sudo apt update &&  sudo apt install -y software-properties-common && sudo apt-add-repository --yes --update ppa:ansible/ansible && sudo apt install -y ansible
  ansible --version
  ```

**注意**

  由于ansible版本更新升级很快，最好是**2.9或者2.10**等以上的版本。

### 所有被管理节点安装python和pip

  使用python3和pip3，需要在InstallationYTung/hosts文件中指定python3的解释器路径，如：

  ```
  which python3
  vim hosts
  <HOST-IP> ansible_python_interpreter=/usr/bin/python3
  ```

### 安装或更新驱动和固件

  * GPU驱动请参考[NVIDIA最新发布的固件链接](https://www.nvidia.cn/geforce/drivers/)
  * NPU和CANN包请参考[华为最新发布的固件工具包链接](https://ascend.huawei.com/zh/#/software/cann/commercial)

## 部署平台

### 修改hosts文件

  * 配置文件：`InstallationYTung/hosts`

  * 主要修改内容如下：

    1. 在 [etcd] 下填写etcd的安装节点（ip）
    2. 在 [kube-master] 下填写k8s集群的master节点（ip）
    3. 在 [kube-worker] 下填写k8s集群的worker节点（ip）。有多个worker节点时，直接换行填写即可。
    4. 在 [nfs-server] 下填写nfs安装节点（ip）
    5. 在 [harbor] 下填写harbor的安装节点（ip)

    ```bash
    # 'etcd' cluster should have odd member(s) (1,3,5,...)
    # variable 'NODE_NAME' is the distinct name of a member in 'etcd' cluster
    [etcd]
    <MASTER-HOST> NODE_NAME=etcd1 
    # master node(s)
    [kube-master]
    <MASTER-HOST>
    # work node(s)
    [kube-worker]
    <WORKER01-HOST>
    <WORKER02-HOST>
    [cluster:children]
    kube-master
    kube-worker
    [nfs-server]
    <MASTER-HOST>
    [harbor]
    <MASTER-HOST> NEW_INSTALL=yes SELF_SIGNED_CERT=yes
    ```

### 修改all.yaml文件

  * 配置文件：InstallationYTung/group_vars/all.yaml

  * 主要修改的内容为：

    1. 项目名称：PROJECT_NAME: "songshanhu"，这个名称将作为集群harbor保存镜像的项目名称
    2. 集群harbor域名：HARBOR_DOMAIN: "harbor.songshanhu.cn"
    3. 每个镜像的name和tag（包括基础镜像和服务镜像）。基础镜像的name和tag改动不多，一般来说，沿用之前的基础镜像name和tag，以及使用之前的镜像tar包就行了。服务镜像一般要修改下，一定要确保imageName、imageTag与推送到集群harbor上的一致。

    ```yaml
    # Project Name
    PROJECT_NAME: "Songshanhu-Inference"
    PLATFORM_NAME: "Apulis-AI-Platform"
    # the name of cluster
    CLUSTER_NAME: "songshanhu"
    # -------- Additional Variables (don't change the default value right now)---
    # Binaries Directory
    i18n: "True"
    bin_dir: "/opt/kube/bin"
    # CA and other components cert/key Directory
    ca_dir: "/etc/kubernetes/ssl"
    # Deploy Directory (aiarts workspace)
    base_dir: "{{ lookup('env', 'PWD') }}"
    # resource directory (include apt, images, dlws code and other package)
    resource_dir: "{{base_dir}}/resources"
    # --------- Main Variables ---------------
    CONTAINER_RUNTIME: "docker"
    # Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn
    CLUSTER_NETWORK: "weavenet"
    # Service proxy mode of kube-proxy: 'iptables' or 'ipvs'
    PROXY_MODE: "iptables"
    # K8S Service CIDR, not overlap with node(host) networking
    SERVICE_CIDR: "10.68.0.0/16"
    # Cluster CIDR (Pod CIDR), not overlap with node(host) networking
    CLUSTER_CIDR: "172.20.0.0/16"
    # NodePort Range
    NODE_PORT_RANGE: "20000-40000"
    # Cluster DNS Domain
    CLUSTER_DNS_DOMAIN: "cluster.local"
    # harbor domain value
    HARBOR_DOMAIN: "harbor.songshanhu.cn"
    # harbor https port
    HARBOR_HTTPS_PORT: 8443
    # CPU Architecture
    arch_map:
      i386: "386"
      x86_64: "amd64"
      aarch64: "arm64"
      armv7l: "armv7"
      armv6l: "armv6"

    thirdparty_images:
      grafana:
        name: "apulistech/grafana"
        tag: "6.7.4"
      grafana-zh:
        name: "apulistech/grafana-zh"
        tag: "6.7.4"
      a910-device-plugin:
        name: "apulistech/a910-device-plugin"
        tag: "devel3"
      alibi-explainer:
        name: ""
      atc:
        name: "apulistech/atc"
        tag: "0.0.1"
      visualjob:
        name: "apulistech/visualjob"
        tag: "1.0"
      tensorflow:
        name: "apulistech/tensorflow"
        tag: "1.14.0-gpu-py3"
      pytorch:
        name: "apulistech/pytorch"
        tag: "1.5"
      mxnet:
        name: "apulistech/mxnet"
        tag: "2.0.0-gpu-py3"
      ubuntu:
        name: "apulistech/ubuntu"
        tag: "18.04"
      bash:
        name: "bash"
        tag: "5"
      k8s-prometheus-adapter:
        name: "directxman12/k8s-prometheus-adapter"
        tag: "v0.7.0"
      tensorflow-serving:
        name: "apulistech/tensorflow-serving"
        tag: "1.15.0"
      tensorrtserver:
        name: ""
      kfserving-pytorchserver:
        name: "apulistech/kfserving-pytorchserver"
        tag: "1.5.1"
      knative-serving:
        name: ""
      kfserving-logger:
        name: ""
      golang:
        name: "golang"
        tag: "1.13.7-alpine3.11"
      prometheus-operator:
        name: "jessestuart/prometheus-operator"
        tag: "v0.38.0"
      coredns:
        name: "k8s.gcr.io/coredns"
        tag: "1.6.7"
      etcd:
        name: "k8s.gcr.io/etcd"
        tag: "3.4.3-0"
      kube-apiserver:
        name: "k8s.gcr.io/kube-apiserver"
        tag: "v1.18.2"
      kube-controller-manager:
        name: "k8s.gcr.io/kube-controller-manager"
        tag: "v1.18.2"
      kube-proxy:
        name: "k8s.gcr.io/kube-proxy"
        tag: "v1.18.2"
      kube-scheduler:
        name: "k8s.gcr.io/kube-scheduler"
        tag: "v1.18.2"
      pause:
        name: "k8s.gcr.io/pause"
        tag: "3.2"
      mysql-server:
        name: "mysql/mysql-server"
        tag: "8.0"
      postgresql:
        name: "postgres"
        tag: "11.10-alpine"
      nvidia-device-plugin:
        name: "nvidia/k8s-device-plugin"
        tag: "1.11"
      kube-vip:
        name: "plndr/kube-vip"
        tag: "0.1.8"
      alertmanager:
        name: "prom/alertmanager"
        tag: "v0.20.0"
      node-exporter:
        name: "prom/node-exporter"
        tag: "v0.18.1"
      prometheus:
        name: "prom/prometheus"
        tag: "v2.18.0"
      redis:
        name: "redis"
        tag: "5.0.6-alpine"
      weave-kube:
        name: "weaveworks/weave-kube"
        tag: "2.7.0"
      weave-npc:
        name: "weaveworks/weave-npc"
        tag: "2.7.0"
      xgbserver:
        name: ""
      sklearnserver:
        name: ""
      onnxruntime-server:
        name: ""
      vc-scheduler:
        name: "vc-scheduler"
        tag: "v0.0.1"
      vc-webhook-manager:
        name: "vc-webhook-manager"
        tag: "v0.0.1"
      vc-controller-manager:
        name: "vc-controller-manager"
        tag: "v0.0.1"
      ascend-k8sdeviceplugin:
        name: "ascend-k8sdeviceplugin"
        tag: "v0.0.1"
      reaper:
        name: ""

    apulis_images:
      apulisvision:
        name: "apulistech/apulisvision"
        tag: "latest"
      aiarts-backend:
        name: "apulistech/aiarts-backend"
        tag: "latest"
      aiarts-frontend:
        name: "dlworkspace_aiarts-frontend"
        tag: "latest"
      custom-user-dashboard-backend:
        name: "dlworkspace_custom-user-dashboard-backend"
        tag: "latest"
      custom-user-dashboard-frontend:
        name: "dlworkspace_custom-user-dashboard-frontend"
        tag: "latest"
      data-platform-backend:
        name: "apulistech/dlworkspace_data-platform-backend"
        tag: "latest"
      gpu-reporter:
        name: "apulistech/dlworkspace_gpu-reporter"
        tag: "latest"
      image-label:
        name: "apulistech/dlworkspace_image-label"
        tag: "latest"
      init-container:
        name: "apulistech/dlworkspace_init-container"
        tag: "latest"
      openresty:
        name: "apulistech/dlworkspace_openresty"
        tag: "latest"
      repairmanager2:
        name: "apulistech/repairmanager2"
        tag: "latest"
      restfulapi2:
        name: "apulistech/restfulapi2"
        tag: "latest"
      webui3:
        name: "dlworkspace_webui3"
        tag: "latest"
      job-exporter:
        name: "apulistech/job-exporter"
        tag: "latest"
      nginx:
        name: "apulistech/nginx"
        tag: "1.9"
      node-cleaner:
        name: "node-cleaner"
        tag: "latest"
      watchdog:
        name: "apulistech/watchdog"
        tag: "1.9"
      istio-proxy:
        name: "apulistech/istio-proxy"
        tag: "latest"
      istio-pilot:
        name: "apulistech/istio-pilot"
        tag: "latest"
      knative-serving-webhook:
        name: "apulistech/knative-serving-webhook"
        tag: "latest"
      knative-serving-queue:
        name: "apulistech/knative-serving-queue"
        tag: "latest"
      knative-serving-controller:
        name: "apulistech/knative-serving-controller"
        tag: "latest"
      knative-serving-activator:
        name: "apulistech/knative-serving-activator"
        tag: "latest"
      knative-serving-autoscaler:
        name: "apulistech/knative-serving-autoscaler"
        tag: "latest"
      knative-net-istio-webhook:
        name: "apulistech/knative-net-istio-webhook"
        tag: "latest"
      knative-net-istio-controller:
        name: "apulistech/knative-net-istio-controller"
        tag: "latest"
      kfserving-manager:
        name: "apulistech/kfserving-manager"
        tag: "latest"
      kfserving-storage-initializer:
        name: "apulistech/kfserving-storage-initializer"
        tag: "latest"
      kfserving-kube-rbac-proxy:
        name: "apulistech/kfserving-kube-rbac-proxy"
        tag: "latest"
      mlflow:
        name: "apulistech/mlflow"
        tag: "v1.0.0"

    ```

### 修改cluster.yaml文件

  * 配置文件：InstallationYTung/group_vars/cluster.yaml

  * 主要修改的内容为：

    1. 集群的kube vip地址：kube_vip_address: "MASTER-HOST>" （使用master的ip即可）

    ```yaml
    MASTER_AS_WORKER: true
    container_mount_path: /dlwsdata
    physical_mount_path: /mntdlws
    manifest_dest: "/root/build"
    user_name: dlwsadmin
    default_cni_path: /opt/cni/bin
    # kube vip address
    kube_vip_address: "<KUBE-VIP-ADDRESS>"
    # cluster api address
    cluster_api_address: "{{ kube_vip_address if kube_vip_address is defined else groups['kube-master'][0]}}"
    # 平台类型，决定了可以启动多少服务
    PLATFORM_MODE: "express"
    datasource: postgres
    # the unified image tag
    image_tag: "latest"
    ```

### 修改harbor.yaml文件

  * 配置文件：InstallationYTung/group_vars/harbor.yaml

  * 主要修改的内容为：

    1. 集群harbor存储数据的路径：HARBOR_LOCATION: "/data"  （建议修改成存储服务器路径）

    ```
    HARBOR_LOCATION: "/data"
    ```

### 执行部署脚本

  按顺序执行下面的命令：
  ```bash
  cd /home/InstallationYTung
  ansible-playbook -i hosts 01.prepare.yaml
  # 配置etcd
  ansible-playbook -i hosts 02.etcd.yaml
  # 配置docker
  ansible-playbook -i hosts 03.docker.yaml
  # 配置kube-master
  ansible-playbook -i hosts 04.kube-master.yaml
  # 配置kube-worker
  ansible-playbook -i hosts 05.kube-worker.yaml
  # 初始化k8s
  ansible-playbook -i hosts 06.kube-init.yaml
  # 配置存储
  ansible-playbook -i hosts 07.storage.yaml
  # 配置harbor
  ansible-playbook -i hosts 08.harbor.yaml
  # 配置网络
  ansible-playbook -i hosts 09.network.yaml
  # 启用服务
  ansible-playbook -i hosts 10.aiarts-service.yaml
  ```

**注意**

1. 每一步的执行结果需要确保`failed=0`
2. 被ignore掉的报错可以忽略

  ```Result:
  <MASTER-HOST>              : ok=21   changed=5    unreachable=0    failed=0    skipped=17   rescued=0    ignored=0
  <WORKER01-HOST>              : ok=24   changed=13   unreachable=0    failed=0    skipped=15   rescued=0    ignored=0
  <WORKER02-HOST>              : ok=21   changed=5    unreachable=0    failed=0    skipped=17   rescued=0    ignored=0
  localhost                  : ok=42   changed=25   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0
  ```

3. 执行`08.harbor.yaml`之后，集群harbor被搭建起来了，harbor用户名和密码如下：

    - 用户名：`admin`
    - 密码：可在`InstallationYTung/credentials/harbor.pwd`中查看

4. `10.aiarts-service.yaml`是用来启动平台所有的服务的，执行完此步没有报错后，**部署就完成了**。接下来就是去测试平台的服务正不正常，以及检查各种pod的状态等。

5. ansible部署遵循幂等性，每一个步骤都可以重复执行。


### 使用管理员账号登录平台

  WEB默认管理员账号为 `admin`, 密码可在`InstallationYTung/credentials/admin.pwd`中查看；之后可参照《快速使用指导》配置平台。

### 更新平台功能组件

1. 修改InstallationYTung/group_vars/all.yaml文件中对应的镜像name和tag
2. 将新的镜像的tar包放在resources/images目录下
  * tar包的名称没有限制（尽量写规范点，能够标识包里的imageName,tag,镜像架构等）
  * tar包里面的镜像的名称有限制：
  * 必须带有/amd64或者/arm64来表示镜像的架构类型，如：
  ```
        harbor.sigsus.cn:8443/sz_gongdianju/apulistech/tensorflow-serving**/amd64**:1.15.0
        harbor.atlas.cn:8443/sz_gongdianju/apulistech/tensorflow-npu**/arm64**:1.15-20.1.RC1
  ```
  * 镜像name和tag必须与all.yaml一致
3. 执行`ansible-playbook -i hosts 08.harbor.yaml`（这个playbook会帮助我们推送新的镜像到集群harbor中）

4. 更新了镜像之后，我们可能需要重拉镜像来**重启某个服务**，方式是：

  ```
  ansible-playbook -i hosts 93.aiarts-restart.yaml -e sn=<SERVICE-NAME>
  # SERVICE-NAME 指的是服务的名称，可在`InstallationYTung/manifests/services`目录下查看
  ```


### 导入预置模型和镜像
    
    请参考**industrial-scene-toolkit **的README中的操作方式执行。
   

## FAQ

1. *iptables: No chain/target/match by that name*

    ```bash
    sudo iptables -t filter -F
    sudo iptables -t filter -X
    systemctl restart docker
    ```

2. *python netaddr包安装失败*
   
    请离线下载whl包，再安装。

3. *etcd服务启动不了或者kube-worker一直不能转为Ready状态*

    ```sh
    TASK [kube-worker : 轮询等待node达到Ready状态] *******************************************************
    FAILED - RETRYING: 轮询等待node达到Ready状态 (8 retries left).
    fatal: [192.168.1.114]: FAILED! => {"attempts": 8, "changed": true, "cmd": "/opt/kube/bin/kubectl get node 192.168.1.114|awk 'NR>1{print $2}'", "delta": "0:00:00.048050", "end": "2021-01-13 18:34:04.558100", "rc": 0, "start": "2021-01-13 18:34:04.510050", "stderr": "Error from server (NotFound): nodes \"192.168.1.114\" not found", "stderr_lines": ["Error from server (NotFound): nodes \"192.168.1.114\" not found"], "stdout": "", "stdout_lines": []}
    fatal: [192.168.1.115]: FAILED! => {"attempts": 8, "changed": true, "cmd": "/opt/kube/bin/kubectl get node 192.168.1.115|awk 'NR>1{print $2}'", "delta": "0:00:00.103329", "end": "2021-01-13 18:34:05.406824", "rc": 0, "start": "2021-01-13 18:34:05.303495", "stderr": "Error from server (NotFound): nodes \"192.168.1.115\" not found", "stderr_lines": ["Error from server (NotFound): nodes \"192.168.1.115\" not found"], "stdout": "", "stdout_lines": []}
    ```

    这个问题是因为之前安装过kubernetes集群，没有reset干净的原因，解决办法：

    `ansible-playbook -i hosts 93.aiarts-restart.yaml`

4. *提示/home/dlwsadmin/.bashrc不存在*

    ```
    TASK [prepare : 写入环境变量$PATH] ***************************************************
    ok: [192.168.1.110] => (item=/root)
    ok: [192.168.1.114] => (item=/root)
    ok: [192.168.1.115] => (item=/root)
    ok: [192.168.1.110] => (item=/home/dlwsadmin)
    failed: [192.168.1.114] (item=/home/dlwsadmin) => {"ansible_loop_var": "item", "changed": false, "item": "/home/dlwsadmin", "msg": "Destination /home/dlwsadmin/.bashrc does not exist !", "rc": 257}
    failed: [192.168.1.115] (item=/home/dlwsadmin) => {"ansible_loop_var": "item", "changed": false, "item": "/home/dlwsadmin", "msg": "Destination /home/dlwsadmin/.bashrc does not exist !", "rc": 257}
    ```

    解决办法：

    ```bash
    touch /home/dlwsadmin/.bashrc
    chown dlwsadmin:dlwsadmin /home/dlwsadmin/.bashrc
    ```
