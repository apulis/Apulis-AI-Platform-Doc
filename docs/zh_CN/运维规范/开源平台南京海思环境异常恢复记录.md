开源平台生产环境异常恢复记录
==============================================================================================


**基本信息：**
* 环境客户：海思
* 发生异常时间： 2020-11-13 ~ 17
* 集群配置： 1 Master(as worker)-NPU + 1 worker-NPU + NFS（master local）
* 资源配置： 8P x 2 nodes NPU （Driver: Rc20.1）

故障记录
----------------------------------------------------------------------------------------------
集群镜像同步下载慢

k8s组件镜像无法从google站点下载

集群使用公网IP，节点无法加入

NPU 网络没有连接

预置镜像和模型的支持

NPU驱动更新


1. 2020-11-13 18:00 左右， 客户反馈平台不可访问


   * 定位分析： Nginx (master) 异常，尝试恢复时发现：`Failed to create pod sandbox: rpc error: code = Unknown desc = failed to inspect sandbox image "k8s.gcr.io/pause:3.2": Error response from daemon: stat /var/lib/docker/overlay2/17ba0ec02fde764d76435757c8d2b255150d98160b305600a603ce40f53cdca8: no such file or directory`
   
   `k8s.gcr.io/pause:3.2` pause 组件会影响集群中所有pod的网络，一旦出现问题将导致平台不可访问!

   * 处理:
      + 重新加载或拉起`k8s.gcr.io/pause:3.2`，操作响应正常，但是nginx还是无法恢复
      + 尝试重置k8s集群，reset 后重新 init;发现init 过程不能正常识别本地镜像，总是会现从k8s.gcr.io拉取，因网络不通该过程受阻
      + 排查发现在本地load备份的k8s组件的镜像文件时有docker缓存文件的冲突，执行docker清理docker system prune -a，并重启docker后，k8s能够init成功，在确认更新代码和镜像后，平台恢复。
   
   * 遗留待处理和优化的问题：
      1. 平台没有对 docker 缓存，日志；npu 日志做管理；使得主机的系统盘空间很快被占用殆尽，海思环境因存储沾满问题已经导致5次平台崩溃；
      2. 平台没有对/tmp，配置依赖环境等管理和监控，没有相应的风险说明和注意事项！
      3. 业务数据存贮在master节点的硬盘上，没有强制要求客户配置存储设备，目前已用90%，暂没有好的处理方式，该问题也严重影响平台使用

        
2. 2020-11-16 20:30 左右， 客户反馈创建任务失败

   * 定位分析： 客户创建的任务被分配在了master节点（master as worker），监视任务拉起过程看到`ContainerCanotRun`容器执行抛错了，对比直接docker run训练镜像发现错误:`docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused "exec: \"bash\": executable file not found in $PATH": unknown. `  ；估计docker相关配置和缓存没有完全恢复，可能需要重置docker环境。

   * 处理：
      使用以下命令彻底删除和清理docker.io然后再重新安装
      ```You have problem with docker.io package.Try
      sudo apt-get remove --purge docker.io 
      sudo apt autoremove 
      rm -rf /var/run/docker.pid
      sudo apt-get install docker.io
      # 重启docker
      sudo systemctl start docker.service
      ```
   * 遗留待处理和优化的问题：
      1. 平台依赖的docker，python，k8s应该有机制保护起来

3. 业涛反馈atlas02， atlas03 外网口不能访问

   * 定位分析： 这是已知问题，可以内部ping通，但是外网口不痛；不确定设备重启，或待机时网口arp在上层接入网口丢失或或是端口协商不起来
    配置网口协商
   * 处理：
      1. 按往常上下电方式只是碰运气，不能有效解决
      2. 配置端口自动协商，外网口能够访问，配置方式参考如下：
   
      ```bash
      # 参考配置示例
      sudo ethtool –s [device_name] speed [10/100/1000] duplex [half/full] autoneg [on/off]
      sudo ethtool –s [enp189s0f0] speed [10/100/1000] duplex [half/full] autoneg [on]
      # 生效配置
      sudo ethtool –s enp189s0f0 speed 1000 duplex full autoneg on
      sudo ethtool –s enp189s0f1 speed 1000 duplex full autoneg on
      ```

4. npu调度插件启用失败显示权限访问目录`/sys/class/devdrv-class`

   * 定位分析：master节点确实没有改目录`/sys/class/devdrv-class`；对比其他atlas做master的环境下存在该目录；并确认了其linux kernel和NPU驱动版本是配套升级好的；
            早晨预期升级kernel，预先安装了新的kernel文件，在恢复平台的重启过程中kernel升级生效了，驱动和工具包需要重新按照。
   * 处理：
       1. master、worker节点清除老的kernel，安装升级版kernel并重启生效
         ```bash
         apt remove --purge Linux-headers-4.15.0-45  &&\ 
         apt remove --purge -y Linux-image-4.15.0-45-generic  &&\
         apt remove --purge -y linux-modules-4.15.0-45-generic  &&\
         apt remove --purge -y Linux-modules-extra-4.15.0-45-generic  &&\
         apt remove --purge -y Linux-tools-4.15.0-45-generic  &&\
         apt remove --purge -y linux-headers-4.15.0-45-generic
         apt autoremove
         apt install -y Linux-headers-4.15.0-99  &&\ 
         apt install -y Linux-image-4.15.0-99-generic  &&\
         apt install -y linux-modules-4.15.0-99-generic  &&\
         apt install -y Linux-modules-extra-4.15.0-99-generic  &&\
         apt install -y Linux-tools-4.15.0-99-generic  &&\
         apt install -y linux-headers-4.15.0-99-generic
         ```
      2. 重启device_plugin
         ```
         ./deploy.py --verbose kubernetes stop a910-device-plugin
         ./deploy.py --verbose kubernetes start a910-device-plugin
         ```

6. worker节点nfs目录没有挂载，执行挂载提示`rpc.nfsd: writing fd to kernel faild: errno 111(connection refused)`

   * 定位分析：nfs 服务的端口2049 111 1305 等是全部放开的；海思那边多次被信息安全部发现要求关闭；之前是希望外网网关屏蔽这些端口，后来我们在机器上配置防火墙，hosts.allow , hosts.deny 限制只走192.168.100/24 私网；这样的配置出现访问拒绝或RPC认证失败异常问题；现在 allow 所有的。这是影响访问或初始挂载的主要原因。 

   * 处理：
      1. 重新安装nfs
      ```
      apt remove --purge -y nfs-kernel-server nfs-common portmap
      sudo apt autoremove 
      sudo apt-get install nfs-kernel-server nfs-common portmap
      sudo ln -s /etc/init.d/nfs-kernel-server /etc/init.d/nfs
      sudo /etc/init.d/nfs-kernel-server restart
      ```
      2. 更新`/etc/hosts.deny`, `/etc/hosts.allow`
      ```bash
      # 配置/etc/hosts.deny禁止所有用户访nfs的守护进程。只有hosts.allow文件指定的ip才能访问。内容如下：

      portmap:ALL
      lockd:ALL
      mountd:ALL
      rquotad:ALL
      statd:ALL

      # 配置/etc/hosts.allow文件允许192.168.100.23访问nfs.添加以下内容：

      portmap: 192.168.100.
      lockd: 192.168.100.
      rquotad: 192.168.100.
      mountd: 192.168.100.
      statd: 192.168.100.
      ```
   * 备注：
      参考链接：https://blog.51cto.com/linux008/551619
      解释/etc/hosts.allow和/etc/hosts.deny配置文件
         这两个文件是指定网络上哪些计算机可以使用nfs服务，该文件的每一个条目录，指定一个服务和一个或一组主机。当一个主机请求nfs服务，它会进行以下操作：

      首先会去检查hosts.allow文件是否匹配里面的条目，如果匹配，则允许访问nsf服务。如果不匹配hosts.allow文件中的条目，服务将会去检查hosts.deny文件，如果匹配，则拒绝访问。如果客户端不匹配两个文件中的条目，则允许访问。
      **注意：hosts.allow中不能有重复的规则,如果有以下重复的条目，则规则生效部分或不确定**
      ```
      portmap: 192.168.100.
      mountd: 192.168.100.
      ```

7. 恢复NPU信息采集
   1. 查看npu信息采集进程在个节点是否存在
   ```
   ps aux| grep npu_info
   ```
   2. 如果不存在或不一致，kill之后，重新起后台进程
   cd apulis_platform/src/ClusterBootstrap
   ./deploy.py --background --sudo runscriptonall scripts/npu/npu_info_gen.py
   ```

8. 恢复NPU log

   ```
   # 更新configmap：
   cd /home/HwHiAiUser/apulis_platform/src/ClusterBootstrap/services/jobmanager2
   kubectl delete -f dlws-scripts.yaml
   rm dlws-scripts.yaml
   ./pre-render.sh
   kubectl create -f dlws-scripts.yaml
   ```

10. netplan apply无效或不响应，机器不能互通

   ```bash
   ifconfig enp189s0f1 192.168.100.23 netmask 255.255.240.0 up
   ip route add default via 207.154.192.1
   sudo nano /etc/resolv.conf
   sudo apt install cloud-init
   sudo nano /etc/resolv.conf
   ```

11. master设备重启后，平台访问页面打不开

   ```
   # 先查看外网访问IP，链接是否正常
   ping 121.37.54.27
   # 在查看default域下nginx的pod状态
   kubectl get pods -o wide 
   # 如果存在，进入pod查看nginx 服务器是否被拉起
   # 确认 /etc/nginx/conf.d/default.conf 中nginx配置，然后重启nginx
   nginx -s stop
   nginx 
   ```

12. 平台创建任务，Job直接Fail 或ErroR
   1. 取确认创建任务使用的模型镜像，是否正确，是否存在，是否可以正常从harbor拉取
   2. 终端查看个节点的nfs挂载目录是否正常
   3. 重启restfulapi, job-manager

13. web打开看到grafana窗口502，或空白
   1. 根据网络情况确认/etc/WEUBUI/local.yaml 或 /etc/Dashboard/local.yaml中grafana的IP

14. 平台查看NPU日志
   
15. 平台数据管理

16. 平台数据清理

17. MySQL数据备份

18. 更新驱动没有更新相应的依赖组件




遗留问题
-------------------------------------------------------------------------------------------

1. master 节点重启将导致平台不可正常使用
   
   1.1. nginx 服务可能出现不能正常拉起
   1.2. worker节点的nfs挂载目录丢失
   1.3. npu采集进程不能自动拉起

2. 网络安全配置
   
   2.1. 屏蔽NFS的外部访问接口

   2.2. 屏蔽Redis外部访问接口

   2.3. 屏蔽其他SSH,MYSQL其他知名端口

3. VC的资源分配策略
   1. 顺序分配
   2. 1:1按比例分配 

4. 日志管理

   平台日志，NPU日志，docker需要定期清理，挂载在非系统盘目录下

5. 在AIArts系统，不支持用户保存预置模型为自定义模型

6. 偶现创建任务Error, 或Fail
   
   * 排除可能原因：

      1. 镜像拉取慢（一键化部署可能出现harbor不能访问的问题）
      2. 架构不适配（x86,arm）
      3. 执行训练的参数或命令

7. 设备网络故障
   1. 设备网口不能访问
   2. 网络带宽不稳定



**参考：**

[1] [ifconfig 配置方法](https://www.digitalocean.com/community/questions/no-internet-connection-after-droplet-reboot )